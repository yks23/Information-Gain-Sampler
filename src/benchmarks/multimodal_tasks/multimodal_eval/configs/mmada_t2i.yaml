# ============================================================
# MMaDA Text-to-Image 生成配置文件
# ============================================================
# 使用方法：
#   python t2i_generate.py config=configs/mmada_t2i.yaml [其他参数覆盖]

wandb:
  entity: null
  resume: 'auto'

experiment:
    project: "mmada-eval"
    name: "mmada-t2i-eval"
    output_dir: "mmada-t2i-eval"

model:
    vq_model:
        type: "magvitv2"
        vq_model_name: "showlab/magvitv2"  # HuggingFace 路径或本地路径

    mmada:
        pretrained_model_path: "./mmada-mix"  # MMaDA 模型路径
        w_clip_vit: False
        new_vocab_size: 134656
        llm_vocab_size: 126464
        codebook_size: 8192
        num_vq_tokens: 1024
        num_new_special_tokens: 0
        tie_word_embeddings: False

    gradient_checkpointing: True

dataset:
    gen_type: "imagenet1k"
    und_type: "captioning"
    combined_loader_mode: "max_size_cycle"
    params:
        train_t2i_shards_path_or_url: ""
        train_mmu_shards_path_or_url: []
        train_lm_shards_path_or_url: ""
        add_caption_prompt: True
        external_caption_path: ""
        external_journeydb_caption_path: ""
        external_laion12m_caption_path: ""
        external_cc12m_caption_path: ""
        validation_prompts_file: "prompts/generation_prompts.txt"
        shuffle_buffer_size: 1000
        num_workers: 4
        resolution: 512
        pin_memory: True
        persistent_workers: True

    preprocessing:
        max_seq_length: 512
        resolution: 512
        center_crop: False
        random_flip: False

optimizer:
    name: adamw
    params:
        learning_rate: 5e-5
        scale_lr: False
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.01
        epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 8000

training:
    gradient_accumulation_steps: 4
    noise_type: "mask"
    batch_size_t2i: 5
    batch_size_lm: 1
    batch_size_mmu: 2
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 10086
    max_train_steps: 500000
    overfit_one_batch: False
    cond_dropout_prob: 0.1
    min_masking_rate: 0.0
    label_smoothing: 0.0
    max_grad_norm: 1
    guidance_scale: 3.5
    generation_timesteps: 50
    t2i_coeff: 1.0
    lm_coeff: 0.1
    mmu_coeff: 1.0

mask_schedule:
    schedule: "cosine"

